\chapter{Controlled Markov Chain Model}
\par{In} the case of complete observations and feedback laws depending only on the current state, then we can call that the state process is a Markov chain.
\section{An Example}
\par{The} state takes values in $R^{n}$, but in many situations it is more appropriate to permit the state to take on only a finite number of values. Consider a machine whose condition at time $k$ is described by the state $x_{k}$ which can take the values $1$ or $2$ with the interpretation that $x_{k} = 1$ or $x_{k} = 2$ depending on whether the machine is in an operational or failed condition. For the moment there are no control actions allowed so that the machine behavior is autonomous. Suppose the machine is operational at time $k$, so $x_{k} = 1$, and there is a probability $q > 0$ that it will fail in the next period, so $x_{k+1} = 2$; with probability $1 - q$ it will continue to remain operational, so $x_{k+1} = 1$.  Suppose further that $q$ does not depend upon previous values $x_{k-1}, \dots, x_{0}$. Finally, suppose that a failed machine continues to remain failed, so that $x_{k+1} = 2$ with probability $1$, if $x_{k} = 2$. Then $\{x_{k}, k > 0\}$ is a Markov chain whose transition probabilities are described by the matrix $\bm{P}=\{\bm{P}_{ij}\}$:
\begin{equation}
	\label{eqn:transion_probabilities}
	\bm{P} = \begin{bmatrix}
		1-q & q \\
		0 & 1  
	 	\end{bmatrix}.
\end{equation}
\noindent The transition probability matrix $\bm{P}$ has the property that \uline{all its elements are non-negative and the sum of the elements in every row is $1$}. Such a matrix is said to be a \textcolor{red}{\textit{stochastic matrix}}. The Markov property is expressed by
\begin{equation}
	\label{eqn:markov_property}
	\text{\textit{Prob}}\{x_{k+1} = j\,|\,x_{k} = i,x_{k-1},\dots,x_{0}\} = \bm{P}_{ij},\,\,i,j \in \{1,2\}.	
\end{equation}
\noindent{\textbf{Control Actions}}:  Let $\bm{u}^{1}_{k}$ denote the intensity of machine use at time $k$.  It takes on values $\bm{u}^{1}_{k}= 0, 1$ or $2$ accordingly as the machine is not used, is in light use, or is in heavy use.  Suppose that the greater the intensity of use, the larger is the likelihood of machine failure. Let $\bm{u}^{2}_{k}$ denote the intensity of machine maintenance effort. Suppose it takes only two values $0$ or $1$, the higher value denoting greater maintenance. The idea is that maintenance reduces the likelihood of machine failure and permits a failed machine to become operational. \newline 
\par{The} effects of these two control actions, intensity of machine use and maintenance, can be modeled as a controlled transition probability as follows. Let $\bm{u}_{k} \doteq (\bm{u}^{1}_{k}, \bm{u}^{2}_{k})$. Then
\begin{align}
	\label{eqn:controlled_transition_probabilities}	
	&\text{\textit{Prob}}\{x_{k+1} = 2\,|\,x_{k} = 1, x_{k-1},\dots,\bm{u}_{k},\bm{u}_{k-1},\dots \} = q_1(\bm{u}^{1}_{k})- q_2(\bm{u}^{2}_{k}) \nonumber \\
	&\text{\textit{Prob}}\{x_{k+1} = 1\,|\,x_{k} = 1, x_{k-1},\dots,\bm{u}_{k},\bm{u}_{k-1},\dots \} = 1 - [q_1(\bm{u}^{1}_{k})- q_2(\bm{u}^{2}_{k})] \nonumber \\
	&\text{\textit{Prob}}\{x_{k+1} = 1\,|\,x_{k} = 2, x_{k-1},\dots,\bm{u}_{k},\bm{u}_{k-1},\dots \} = q_2(\bm{u}^{2}_{k}) \nonumber \\
	&\text{\textit{Prob}}\{x_{k+1} = 2\,|\,x_{k} = 2, x_{k-1},\dots,\bm{u}_{k},\bm{u}_{k-1},\dots \} = 1 - q_2(\bm{u}^{2}_{k}).
\end{align}
\par{These} transition probabilities can be put in matrix form similar to \ref{eqn:transion_probabilities}, except that they will be functions of the control $\bm{u}$:
\begin{equation}
	\label{eqn:controlled_transion_probabilities_matrix}
	\bm{P}(\bm{u^{1}},\bm{u^{2}}) = \begin{bmatrix}
		1-q_{1}(\bm{u}^{1})+q_{2}(\bm{u}^{2}) & q_{1}(\bm{u}^{1})-q_{2}(\bm{u}^{2}) \\
		q_{2}(\bm{u}^{2}) & 1-q_{2}(\bm{u}^{2})  
	\end{bmatrix}.
\end{equation}
\par{Equation} \ref{eqn:controlled_transition_probabilities} is illustrated in the state transition diagram below.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.55\textwidth,keepaspectratio]{Figures/State_Transition_Diagram/state_transition_diagram.pdf}
	\caption{The state transition diagram of a controlled Markov process.} 
	\label{fig:state_transition_diagram_of_controlled_state}	
\end{figure}
\par{Of} course the values of $q$ are such that $q_{1}(0)<q_{1}(1)<q_{1}(2)$ and $q_{2}(0)<q_{2}(1)$ because a lightly used or better maintained machine is less likely to fail than a heavily used or less well-maintained machine. We also expect that the last two probabilities in \ref{eqn:controlled_transition_probabilities} should not depend on $\bm{u}^{1}_{k}$ because when the machine has failed, it cannot be used. \newline
\par{Suppose} the state is observed and consider a feedback policy $\{\bm{g}_{0},\bm{g}_{1},\dots\}$ which is time-invariant, that is, $\bm{g}_{k} \equiv \bm{g}$, and let $\bm{u}_{k} = \bm{g}(x_{k})$. This results in the transition probability matrix $\bm{P}^{\bm{g}} = \{\bm{P}^{g}_{ij}\}$ where 
\begin{equation}
	\label{eqn:transion_probabilities__with_feedback_policy}
	\bm{P}^{\bm{g}}_{ij} \doteq \bm{P}_{ij}\big(\bm{g}(i)\big),\,\,i,j \in \{1,2\}.
\end{equation}
\par For example, if $\bm{g}(1) = (2, 0)$, i.e the machine is in heavy use and smaller maintenance is required, and $\bm{g}(2) = (0,1)$, the machine is not used and greater maintenance is required. Then
\begin{equation}
	\label{eqn:controlled_transion_probabilities_matrix_with_feedback_policy}
	\bm{P}^{\bm{g}} = \begin{bmatrix}
		1-q_{1}(2)+q_{2}(0) & q_{1}(2)-q_{2}(0) \\
		q_{2}(1) & 1-q_{2}(1)  
	\end{bmatrix}.
\end{equation}
\par{The} resulting process $\{x_{k}\}$ is a Markov chain with stationary transition probability $\bm{P}^{\bm{g}}$. The joint probability distribution of $x_{k}$ can be written as the row vector
\begin{equation}
	\label{eqn:probability_distribution_of_x_vector}
	\nonumber
	\bm{p}_{k} \doteq (\text{\textit{Prob}}\{x_{k}=1\},\text{\textit{Prob}}\{x_{k}=2\}).
\end{equation}
\par{By} the Markov property \ref{eqn:controlled_transition_probabilities}
\begin{equation}
	\label{eqn:probability_distribution_update}
	\bm{p}_{k+m} \doteq \bm{p}_{k}[\bm{P}^{\bm{g}}]^{m},\,\,m\geq0,
\end{equation}
and, in particular,
\begin{equation}
	\label{eqn:probability_distribution_update_II}
	\bm{p}_{k} \doteq \bm{p}_{0}[\bm{P}^{\bm{g}}]^{k},
\end{equation}
\noindent{where} $\bm{p}_{0}$ is the initial distribution of $x_{0}$. \newline
\par Often, \uline{as $k\rightarrow\infty$, $\bm{p}_{k}$ converges to a probability distribution $\bm{p}=(\bm{p}(1),\bm{p}(2))$ that does not depend on the initial distribution $\bm{p}_{0}$.} We then say that it is an \textcolor{red}{\textit{ergodic}} chain. The limiting probability distribution is called the \textcolor{red}{\textit{steady-state}} or \uline{\textit{equilibrium}} or \uline{\textit{invariant}} distribution. It is the solution of the following linear equations
\begin{equation}
	\begin{aligned}
		\label{eqn:steady_state_transition}
		&\bm{p} = \bm{p}\bm{P}^{\bm{g}},  \\
		&\bm{p}(1) + \bm{p}(2) = 1. \\
	\end{aligned}
\end{equation}
\par{Equation} \ref{eqn:steady_state_transition} always has a solution.  In the ergodic case the solution is unique and the limiting distribution $\bm{p}=\big(\bm{p}(1),\bm{p}(2)\big)$ has the following interpretation:
\begin{equation}
	\label{eqn:interpretation_of_the_ergodic_chain}
	\bm{p}(i) = \lim_{n\rightarrow{\infty}}\frac{1}{n}\sum_{k=1}^{n}I(x_{k}=i)\,\,\text{with probability}\,\,1,
\end{equation}
\noindent{where} $I$ is the \textcolor{red}{\textit{indicator function}}â€”i.e., $I(x_{k}=i) = 1$ if $x_{k} = i$, and $I(x_{k}=i) = 0$ if $x_{k} \neq i$. Thus $\bm{p}(1)$  is the average proportion of time that the machine is operational and $\bm{p}(2)$ the average proportion of time it spends in the failed state. \newline
\par From \ref{eqn:steady_state_transition} it is evident that the steady state probability $\bm{p}$ depends on the feedback law $g$. So by changing the policy $g$, that is, by changing the use and maintenance of the machine, we can alter the number of times it fails. One may then ask the following question: Which policy $\bm{g}$ leads to the best p?
\section{Finite state controlled Markov chains}
\par{The} preceding example generalizes to the case of an arbitrary finite state controlled Markov chain whose state $x_{k}$ takes values in $\{1,\dots,I\}$. The control $\bm{u}_{k}$ takes values in a pre-specified set $\bm{U}$. $\bm{U}$ may be finite or infinite. The transition probabilities are specified by the $I \times I$ matrix valued function on $\bm{U}$,
\begin{equation}
	\label{eqn:finite_state_transition_probabilities}
	\bm{u} \rightarrow \bm{P}(\bm{u}) \doteq \{\bm{P}_{ij}(\bm{u}),\,\,1\leq{i},j\leq{I}\}
\end{equation}
\noindent{with} the interpretation that
\begin{equation}
	\label{eqn:finite_state_markov_property}
	\text{\textit{Prob}}\{x_{k+1} = j\,|\,x_{k} = i,x_{k-1},\dots,x_{0},\bm{u}_{k},\dots,\bm{u}_{0}\} = \bm{P}_{ij}(\bm{u}_{k}).	
\end{equation}
\par \uline{The matrix $\bm{P}(\bm{u})$ has the property that every element is non-negative, and the sum of the elements in every row is $1$ - i.e., it is a stochastic matrix.} We must also specify the probability distribution of the initial state $x_{0}$. In case the observation $y_{k}\,\cancel{\equiv}\,x_{k}$, one must also specify the observation probability
\begin{equation}
	\label{eqn:finite_state_observation_probability}
	P(y|i) \doteq \text{\textit{Prob}}\{y_{k} = y\,{|}\, x_{k} = i\}.
\end{equation}
\section{Complete observations and Markov policies}
Consider the controlled Markov chain model \ref{eqn:finite_state_transition_probabilities}. Suppose the state is observed, $y_{k} \equiv x_{k}$. Let $\bm{g} = \{\bm{g}_{0},\bm{g}_{1},\dots\}$ be a feedback policy such that $\bm{g}_{k}$ depends only on the current state $x_{k}$ (and not on $x_{k-1}$, $x_{k-2}$, $\dots$). \uline{We call such a $\bm{g}$ a \textcolor{red}{\textit{Markov policy}}}. \newline
\par Let $\bm{g}$ be a Markov policy, and let $\{x_{k}\}$ be the resulting state process. Denote the probability distribution of $x_{k}$ by the $I$-dimensional row vector
\begin{equation}
	\label{eqn:I_dimensional_Markov_Policy}
	\bm{p}^{\bm{g}}_{k} \doteq (\text{\textit{Prob}}\{x_{k}=1\},\dots,\text{\textit{Prob}}\{x_{k}=I\}),
\end{equation}
\noindent{the} superscript $\bm{g}$ emphasizes the dependence on $\bm{g}$. However, in the sequel we drop the superscript since the $\bm{g}$-dependence will be clear from the context. \newline
\par\noindent\textcolor{red}{\textit{Lemma}}: When a Markov policy $\bm{g}$ is employed, the resulting state process $\{x_{k}\}$ is a Markov process. Its one-step transition probability at time $k$ is given by the matrix
\begin{equation}
	\label{eqn:one_step_transtion_probability}
	\nonumber
	\bm{P}^{\bm{g}}_{k} \doteq \{(\bm{P}^{\bm{g}}_{k})_{ij} \doteq \bm{P}_{ij}\big(\bm{g}_{k}(i)\big),\,\,1 \leq i,j \leq I)\},
\end{equation}
\noindent{Its} $m$-step transition probability at time $k$ is given by the matrix
\begin{equation}
	\label{eqn:m-step_transtion_probability}
	\nonumber
	\bm{P}^{\bm{g}}_{k} {\times}{\dots}{\times} \bm{P}^{\bm{g}}_{k+m-1},
\end{equation}
so its $ij$-th element is the probability that the state will be $j$ at time $k+m$ given that it is $i$ at time $k$. Hence
\begin{equation}
	\label{eqn:m-step_transtion_probability_II}
	\nonumber
	\bm{p}_{k+m} = \bm{p}_{k}(\bm{P}^{\bm{g}}_{k} {\times}{\dots}{\times} \bm{P}^{\bm{g}}_{k+m-1}).
\end{equation}
In particular,
\begin{equation}
	\label{eqn:k-step_transtion_probability}
	\bm{p}_{k} = \bm{p}_{0}(\bm{P}^{g}_{0} {\times}{\dots}{\times} \bm{P}^{g}_{k-1}),
\end{equation}
\noindent{where} $\bm{p}_{0}$ is the probability distribution of the initial state $x_{0}$. \newline
\par\noindent\textcolor{red}{\textit{Proof}}: The proof is immediate from the Markov property \ref{eqn:finite_state_markov_property}. Since the transition probability matrix $\bm{P}^{\bm{g}}_{k}$ depends on the time $k$, we say that $\{x_{k}\}$ is a Markov chain with nonstationary transition probability.
\section{The cost of a Markov policy}
\par A Markov policy $\bm{g}$ determines the probability distribution of the state process $\{x_{k}\}$ and the control process $\{\bm{u}_{k} = \bm{g}_{k}(x_{k})\}$. Different policies will lead to different probability distributions. \uline{In optimal control problems one is interested in finding the best or optimal policy. To do this one needs to compare different policies. This is done by specifying a \textcolor{red}{\textit{cost function}}.} This is a sequence of real valued functions of the state and control,
\begin{equation}
	\label{eqn:cost_function_of_I_states}
	\nonumber
	c_{k}(i,\bm{u}),\,\,1 \leq i \leq I,\,\, \bm{u} \in \bm{U},\,\,k \geq 0.
\end{equation}
\par{The} interpretation is that $c_{k}(i,\bm{u})$ is the cost to be paid if at time $k$, $x_{k} = i$ and $\bm{u}_{k} = \bm{u}$. Fix a Markov policy $\bm{g}$. The cost incurred by $\bm{g}$ up to the time horizon $N$ is $\sum_{k=0}^{N}c_{k}(x_{k},\bm{u}_{k})$. This is a random variable since $x_{k}$ and $\bm{u}_{k}$ are random. Hence the expected cost is
\begin{equation}
	\label{eqn:expected_cost_up_to_time_horizion_N}
	J(\bm{g}) \doteq \mathbb{E}^{\bm{g}}\bigg[\sum_{k=0}^{N}c_{k}(x_{k},\bm{u}_{k})\bigg] = \mathbb{E}^{\bm{g}}\bigg[\sum_{k=0}^{N}c_{k}\big(x_{k},\bm{g}_{k}(x_{k})\big)\bigg],
\end{equation}
\noindent{here} $\mathbb{E}^{\bm{g}}$ denotes expectation with respect to the probability distribution of $\{x_{k}\}$, $\{\bm{u}_{k}\}$ determined by $\bm{g}$. $J(\bm{g})$ can be readily evaluated in terms of the transition probability matrices $\bm{P}^{\bm{g}}_{k}$ as follows. From \ref{eqn:expected_cost_up_to_time_horizion_N} it can be obtained
\begin{equation}
	\begin{aligned}
		\label{eqn:expected_cost_up_to_time_horizion_N_in_terms_of_P_matrices}
		J(\bm{g}) &= \sum_{k=0}^{N}\sum_{i=1}^{I}\text{\textit{Prob}}\{x_{k}=i\}c_{k}\big(i,\bm{g}_{k}(i)\big) \\
		&= \sum_{k=0}^{N} \bm{p}_{k}\,\bm{c}^{\bm{g}}_{k} = \sum_{k=0}^{N} \bm{p}_{0}\,(\bm{P}^{\bm{g}}_{0} \times \, \dots \, \times \bm{P}^{\bm{g}}_{k-1}) \bm{c}^{\bm{g}}_{k}, \\
	\end{aligned}
\end{equation}
\noindent{where} $\bm{c}^{\bm{g}}_{k}$ is the $I$-dimensional column vector
\begin{equation}
	\label{eqn:cost_values_with_column_vector_at_time_k}
	\bm{c}^{\bm{g}}_{k} \doteq \bigg(c_{k}\big(1,\bm{g}_{k}(1)\big),\,\dots,\, c_{k}\big(I,\bm{g}_{k}(I)\big)\bigg)^{T}.
\end{equation}
\par{The} last equality in \ref{eqn:expected_cost_up_to_time_horizion_N_in_terms_of_P_matrices} follows from \ref{eqn:k-step_transtion_probability}. Thus the best Markov policy $\bm{g}$ is the one that minimizes $J(\bm{g}) = \sum_{k=0}^{N} \bm{p}_{0}\,(\bm{P}^{\bm{g}}_{0} \times \, \dots \, \times \bm{P}^{g}_{k-1}) \bm{c}^{g}_{k}$. \uline{A \textit{Dynamic Programming} can be an approach for computing the best $\bm{g}$.} Central to dynamic programming is a recursive technique for calculating the cost of a Markov policy $\bm{g}$. Since the technique depends only on the fact that the state process corresponding to $\bm{g}$ is Markov, which is introduced here. For each time $1 \leq k \leq N$, and state $1 \leq i \leq I$, let $V^{\bm{g}}_{k}(i)$ denote the expected cost incurred during $k,\,\dots,\,N$ when $x_{k} = i$.  That is,
\begin{equation}
	\label{eqn:expected_cost_incurred_during_k_to_N}
	V^{\bm{g}}_{k}(i) \doteq \mathbb{E}^{\bm{g}}\bigg[\sum_{l=k}^{N}c_{l}\big(x_{l},\bm{g}_{l}(x_{l})\big)\, \bigg| x_{k} = i \bigg] .
\end{equation}
\par{Observe} that with this notation the total cost \ref{eqn:expected_cost_up_to_time_horizion_N_in_terms_of_P_matrices} is
\begin{equation}
	\label{eqn:expected_total_cost}
	J(\bm{g}) = \sum_{i=1}^{I} (\bm{p}_{0})_{i} \, V^{\bm{g}}_{0}(i).
\end{equation}
\par\noindent\textcolor{red}{\textit{Lemma}}: The functions $V^{\bm{g}}_{k}(i)$ can be calculated by backward recursion,
\begin{equation}
	\label{eqn:total_cost_update_backward_recursion}
	V^{\bm{g}}_{k}(i) = c_{k}\big(i,\bm{g}_{k}(i)\big) + \sum_{j=1}^{I} \big(\bm{P^{\bm{g}}_{k}}\big)_{ij} \, V^{\bm{g}}_{k+1}(j),\,\,0 \leq k < N.
\end{equation}
\noindent{starting} with the final condition
\begin{equation}
	\label{eqn:total_cost_update_final_condition}
	V^{\bm{g}}_{N}(i) = c_{N}\big(i,\bm{g}_{N}(i)\big).
\end{equation}
\par\noindent\textcolor{red}{\textit{Proof}}: 